{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonackermann/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:18:44.163389: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-26 10:18:44.163528: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 - set up data pipiline\n",
    "def prepare_mnist_data(mnist):\n",
    "    # map from uint8 to tf.float\n",
    "    mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), (tf.cast(target, tf.float32))))\n",
    "\n",
    "    # flatten input\n",
    "    mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1, )), target))\n",
    "    \n",
    "    # normalize input to gaussian distribution or divide by 128\n",
    "    mnist = mnist.map(lambda img, target: (((img/128)-1), target))\n",
    "\n",
    "    # zip two images together\n",
    "    #a = mnist.take(30000)\n",
    "    #b = mnist.skip(30000).take(30000)\n",
    "    #a, b = tf.split(mnist, 2, 1)\n",
    "    #a = mnist.shard(num_shards=2, index=0)\n",
    "    #b = mnist.shard(num_shards=2, index=1)\n",
    "    mnist = tf.data.Dataset.zip((mnist.shuffle(2000), mnist.shuffle(2000)))\n",
    "    mnist = mnist.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
    "    zipped_ds = mnist.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # create target for subtask 1\n",
    "\n",
    "    # keep the progess in memory\n",
    "    mnist = mnist.cache()\n",
    "    mnist = mnist.shuffle(1000) \n",
    "    mnist = mnist.batch(32) # 32 image in one batch\n",
    "    mnist = mnist.prefetch(20) # prepare 20 next datapoints \n",
    "\n",
    "    return mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds.apply(prepare_mnist_data)\n",
    "test_dataset = test_ds.apply(prepare_mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 - build network\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense_input1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense_input2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        print(inputs.shape)\n",
    "        # shared weight model\n",
    "        #input1 = self.dense_input1(inputs[0]) # still to decide if this is the right data\n",
    "        #input2 = self.dense_input2(inputs[1]) \n",
    "        #input3 = tf.keras.layers.ReLU(tf.math.add(input1, input2))\n",
    "        #x = self.dense1(input3)\n",
    "        #x = self.dense2(x)\n",
    "        #x = self.out(x)\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 - training the network\n",
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "\n",
    "    # loss object and optimizer and are instances of respective tensorflow classes \n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables) # all variables with trainable = True\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # updating weights with optimizer\n",
    "    return loss\n",
    "\n",
    "# 2.4.1 - testing the model\n",
    "def test(model, test_data, loss_function):\n",
    "\n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = [] # continuous\n",
    "\n",
    "    print(test_data)\n",
    "    # input is batch of 32 examples\n",
    "    for input1,input2, target in test_data:\n",
    "        prediction = model(input)\n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:17:15.529435: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:17:15.529693: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:17:15.532801: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m test_accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[39m# test model before training\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m test(model, test_dataset, cross_entropy_loss)\n\u001b[1;32m     17\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss)\n\u001b[1;32m     18\u001b[0m test_accuracies\u001b[39m.\u001b[39mappend(test_accuracy)\n",
      "Cell \u001b[0;32mIn [92], line 20\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     17\u001b[0m test_loss_aggregator \u001b[39m=\u001b[39m [] \u001b[39m# continuous\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# input is batch of 32 examples\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, target \u001b[39min\u001b[39;00m test_data:\n\u001b[1;32m     21\u001b[0m     prediction \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39m)\n\u001b[1;32m     22\u001b[0m     sample_test_loss \u001b[39m=\u001b[39m loss_function(target, prediction)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Running the network\n",
    "# Training\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyModel()\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# for visualization\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# test model before training\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
    "\n",
    "    # training \n",
    "    epoch_loss_agg = []\n",
    "    for input, target in train_dataset:\n",
    "        print(input.shape)\n",
    "        #train_loss = train_step(model, input,target, cross_entropy_loss, optimizer)\n",
    "        #epoch_loss_agg.append(train_loss)\n",
    "\n",
    "    # track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    # track accuracy and test loss\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "#visualization(train_losses, test_losses, test_accuracies)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.bool, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:18:24.366533: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:18:24.366789: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:18:24.369865: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cross_entropy_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy()\n\u001b[1;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(learning_rate)\n\u001b[0;32m----> 7\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m test(model, test_dataset, cross_entropy_loss)\n",
      "Cell \u001b[0;32mIn [97], line 21\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(test_data)\n\u001b[1;32m     20\u001b[0m \u001b[39m# input is batch of 32 examples\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m (input1,input2), target \u001b[39min\u001b[39;00m test_data:\n\u001b[1;32m     22\u001b[0m     prediction \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39m)\n\u001b[1;32m     23\u001b[0m     sample_test_loss \u001b[39m=\u001b[39m loss_function(target, prediction)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model = MyModel()\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3449bbb043929c6f13b514689ff91c66257e0787e2d8bb0eba8270d3f40eacf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
