{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 10:18:44.163389: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-26 10:18:44.163528: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 - set up data pipiline\n",
    "def prepare_mnist_data(mnist):\n",
    "    # map from uint8 to tf.float\n",
    "    mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), (tf.cast(target, tf.float32))))\n",
    "\n",
    "    # flatten input\n",
    "    mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1, )), target))\n",
    "    \n",
    "    # normalize input to gaussian distribution or divide by 128\n",
    "    mnist = mnist.map(lambda img, target: (((img/128)-1), target))\n",
    "\n",
    "    # zip two images together\n",
    "    #a = mnist.take(30000)\n",
    "    #b = mnist.skip(30000).take(30000)\n",
    "    #a, b = tf.split(mnist, 2, 1)\n",
    "    #a = mnist.shard(num_shards=2, index=0)\n",
    "    #b = mnist.shard(num_shards=2, index=1)\n",
    "    mnist = tf.data.Dataset.zip((mnist.shuffle(2000), mnist.shuffle(2000)))\n",
    "    mnist = mnist.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
    "    zipped_ds = mnist.map(lambda x1, x2, t: (x1,x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # create target for subtask 1\n",
    "\n",
    "    # keep the progess in memory\n",
    "    mnist = mnist.cache()\n",
    "    mnist = mnist.shuffle(1000) \n",
    "    mnist = mnist.batch(32) # 32 image in one batch\n",
    "    mnist = mnist.prefetch(20) # prepare 20 next datapoints \n",
    "\n",
    "    return mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds.apply(prepare_mnist_data)\n",
    "test_dataset = test_ds.apply(prepare_mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 - build network\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense_input1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense_input2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, input1, input2):\n",
    "        input1 = self.dense_input1(input1) # still to decide if this is the right data\n",
    "        input2 = self.dense_input2(input2) \n",
    "        input3 = tf.math.add(input1, input2)\n",
    "        print(input3.shape)\n",
    "        x = self.dense1(input3)\n",
    "        x = self.dense2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 - training the network\n",
    "def train_step(model, input1, input2, target, loss_function, optimizer):\n",
    "\n",
    "    # loss object and optimizer and are instances of respective tensorflow classes \n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input1, input2)\n",
    "        loss = loss_function(target, prediction)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables) # all variables with trainable = True\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # updating weights with optimizer\n",
    "    return loss\n",
    "\n",
    "# 2.4.1 - testing the model\n",
    "def test(model, test_data, loss_function):\n",
    "\n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = [] # continuous\n",
    "\n",
    "    # input is batch of 32 examples\n",
    "    for input1,input2, target in test_data:\n",
    "        prediction = model(input1, input2)\n",
    "        sample_test_loss = loss_function(target, prediction)\n",
    "        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
    "        sample_test_accuracy = np.mean(sample_test_accuracy)\n",
    "        test_loss_aggregator.append(sample_test_loss.numpy())\n",
    "        test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
    "\n",
    "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
    "    test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.bool, name=None))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:26:59.584221: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:26:59.584942: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:26:59.588724: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling layer \"my_model_9\" \"                 f\"(type MyModel).\n\nin user code:\n\n    File \"/var/folders/2q/hwnn9141093b7bkbnd4bm7tc0000gn/T/ipykernel_20812/1643765698.py\", line 16, in call  *\n        input3 = tf.keras.layers.ReLU(tf.math.add(input1, input2))\n    File \"/Users/leonackermann/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/layers/activation/relu.py\", line 79, in __init__  **\n        if max_value is not None and max_value < 0.0:\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\n\nCall arguments received by layer \"my_model_9\" \"                 f\"(type MyModel):\n  • input1=tf.Tensor(shape=(32, 784), dtype=float32)\n  • input2=tf.Tensor(shape=(32, 784), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m test_accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[39m# test model before training\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m test(model, test_dataset, cross_entropy_loss)\n\u001b[1;32m     17\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss)\n\u001b[1;32m     18\u001b[0m test_accuracies\u001b[39m.\u001b[39mappend(test_accuracy)\n",
      "Cell \u001b[0;32mIn [110], line 22\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# input is batch of 32 examples\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m input1,input2, target \u001b[39min\u001b[39;00m test_data:\n\u001b[0;32m---> 22\u001b[0m     prediction \u001b[39m=\u001b[39m model(input1, input2)\n\u001b[1;32m     23\u001b[0m     sample_test_loss \u001b[39m=\u001b[39m loss_function(target, prediction)\n\u001b[1;32m     24\u001b[0m     sample_test_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(target, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39margmax(prediction, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1233\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1233\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1234\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1235\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling layer \"my_model_9\" \"                 f\"(type MyModel).\n\nin user code:\n\n    File \"/var/folders/2q/hwnn9141093b7bkbnd4bm7tc0000gn/T/ipykernel_20812/1643765698.py\", line 16, in call  *\n        input3 = tf.keras.layers.ReLU(tf.math.add(input1, input2))\n    File \"/Users/leonackermann/miniforge3/envs/iannwtf/lib/python3.10/site-packages/keras/layers/activation/relu.py\", line 79, in __init__  **\n        if max_value is not None and max_value < 0.0:\n\n    OperatorNotAllowedInGraphError: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n\n\nCall arguments received by layer \"my_model_9\" \"                 f\"(type MyModel):\n  • input1=tf.Tensor(shape=(32, 784), dtype=float32)\n  • input2=tf.Tensor(shape=(32, 784), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Running the network\n",
    "# Training\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MyModel()\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# for visualization\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# test model before training\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "test_losses.append(test_loss)\n",
    "test_accuracies.append(test_accuracy)\n",
    "\n",
    "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
    "\n",
    "    # training \n",
    "    epoch_loss_agg = []\n",
    "    for input1,input2,target in train_dataset:\n",
    "        print(input.shape)\n",
    "        #train_loss = train_step(model, input1, input2, target, cross_entropy_loss, optimizer)\n",
    "        #epoch_loss_agg.append(train_loss)\n",
    "\n",
    "    # track training loss\n",
    "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
    "\n",
    "    # track accuracy and test loss\n",
    "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "#visualization(train_losses, test_losses, test_accuracies)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None, 784), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.bool, name=None))>\n",
      "(32, 256)\n",
      "(32, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 13:30:39.190093: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-11-26 13:30:39.223688: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:30:39.225201: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-11-26 13:30:39.228564: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [116], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cross_entropy_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy()\n\u001b[1;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(learning_rate)\n\u001b[0;32m----> 7\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m test(model, test_dataset, cross_entropy_loss)\n",
      "Cell \u001b[0;32mIn [110], line 24\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_data, loss_function)\u001b[0m\n\u001b[1;32m     22\u001b[0m prediction \u001b[39m=\u001b[39m model(input1, input2)\n\u001b[1;32m     23\u001b[0m sample_test_loss \u001b[39m=\u001b[39m loss_function(target, prediction)\n\u001b[0;32m---> 24\u001b[0m sample_test_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(target, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39margmax(prediction, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m sample_test_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(sample_test_accuracy)\n\u001b[1;32m     26\u001b[0m test_loss_aggregator\u001b[39m.\u001b[39mappend(sample_test_loss\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniforge3/envs/iannwtf/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model = MyModel()\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3449bbb043929c6f13b514689ff91c66257e0787e2d8bb0eba8270d3f40eacf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
